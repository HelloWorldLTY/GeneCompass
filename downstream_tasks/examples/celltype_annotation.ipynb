{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeneCompass Fine-Tuning for Cell-type Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For human-specific and mouse-specific tasks, we compared pre-trained GeneCompass with GeneCompass without pre-training and Geneformer on human multiple sclerosis (hMS), lung (hLung) and liver (hLiver) datasets, and mouse brain (mBrain), lung (mLung) and pancreas (mPancreas) datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model for cell-type annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-10 20:51:49,280] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "# Choose the GPU to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import pickle\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import Trainer\n",
    "from genecompass import BertForSequenceClassification\n",
    "from transformers.training_args import TrainingArguments\n",
    "from genecompass import DataCollatorForCellClassification\n",
    "from genecompass.utils import load_prior_embedding\n",
    "import argparse\n",
    "import numpy as np\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token dict path\n",
    "token_dictionary_path='../../prior_knowledge/human_mouse_tokens.pickle'\n",
    "\n",
    "# load knowledges\n",
    "knowledges = dict()\n",
    "out = load_prior_embedding(token_dictionary_or_path=token_dictionary_path)\n",
    "knowledges['promoter'] = out[0]\n",
    "knowledges['co_exp'] = out[1]\n",
    "knowledges['gene_family'] = out[2]\n",
    "knowledges['peca_grn'] = out[3]\n",
    "knowledges['homologous_gene_human2mouse'] = out[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SV2C-expressing interneuron': 0, 'mixed glial cell?': 1, 'VIP-expressing interneuron': 2, 'PVALB-expressing interneuron': 3, 'microglial cell': 4, 'astrocyte': 5, 'SST-expressing interneuron': 6, 'oligodendrocyte precursor cell': 7, 'mixed excitatory neuron': 8, 'pyramidal neuron?': 9, 'cortical layer 2-3 excitatory neuron B': 10, 'phagocyte': 11, 'oligodendrocyte C': 12, 'endothelial cell': 13, 'cortical layer 2-3 excitatory neuron A': 14, 'cortical layer 4 excitatory neuron': 15, 'cortical layer 5-6 excitatory neuron': 16, 'oligodendrocyte A': 17}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffc5ed7d11947bf863cb5cdae9f4cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/7697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2aafa9a1584af2b042516e1dbba60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/13244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa216b08035f4201b4a72546f29aedbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=16):   0%|          | 0/13244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data path\n",
    "train_path = '../../data/cell_type_annotation/hMS/train'\n",
    "test_path =  '../../data/cell_type_annotation/hMS/test'\n",
    "\n",
    "# load datasets\n",
    "train_set = load_from_disk(train_path)\n",
    "test_set = load_from_disk(test_path)\n",
    "\n",
    "# rename columns\n",
    "train_set = train_set.rename_column(\"celltype\", \"label\")\n",
    "test_set = test_set.rename_column(\"celltype\", \"label\")\n",
    "\n",
    "# create dictionary of cell types : label ids\n",
    "target_names = set(list(Counter(train_set[\"label\"]).keys()) + list(Counter(test_set[\"label\"]).keys()))\n",
    "target_name_id_dict = dict(zip(target_names, [i for i in range(len(target_names))]))\n",
    "print(target_name_id_dict)\n",
    "\n",
    "# change labels to numerical ids\n",
    "def classes_to_ids(example):\n",
    "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "    return example\n",
    "train_set = train_set.map(classes_to_ids, num_proc=16)\n",
    "test_set = test_set.map(classes_to_ids, num_proc=16)\n",
    "\n",
    "# filter dataset for cell types in corresponding training set\n",
    "trained_labels = list(Counter(train_set['label']).keys())\n",
    "def if_trained_label(example):\n",
    "    return example['label'] in trained_labels\n",
    "test_set = test_set.filter(if_trained_label, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute metrics for cell-type annotation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # calculate accuracy and macro f1 using sklearn's function\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, average=\"macro\")\n",
    "    recall = recall_score(labels, preds, average=\"macro\")\n",
    "    macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrained_models/GeneCompass_Base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls4value.predictions.decoder.bias', 'cls4value.predictions.transform.LayerNorm.weight', 'cls4value.predictions.bias', 'cls4value.predictions.transform.LayerNorm.bias', 'cls4value.predictions.decoder.weight', 'emb_warmup.steps', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls4value.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'emb_warmup.alpha', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls4value.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../pretrained_models/GeneCompass_Base and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): KnowledgeBertEmbeddings(\n",
      "      (word_embeddings): Embedding(50558, 768, padding_idx=0)\n",
      "      (promoter_embeddings): PriorEmbedding(\n",
      "        (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (co_exp_embeddings): PriorEmbedding(\n",
      "        (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (gene_family_embeddings): PriorEmbedding(\n",
      "        (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (peca_grn_embeddings): PriorEmbedding(\n",
      "        (linear1): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (concat_embeddings): Sequential(\n",
      "        (cat_fc): Linear(in_features=3841, out_features=768, bias=True)\n",
      "        (cat_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (cat_gelu): QuickGELU()\n",
      "        (cat_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (position_embeddings): Embedding(2048, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.02, inplace=False)\n",
      "    )\n",
      "    (cls_embedding): Embedding(2, 768)\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.02, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.02, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.02, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.02, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# pretrain checkpoint path\n",
    "checkpoint_path='../../pretrained_models/GeneCompass_Base'\n",
    "\n",
    "# set freeze layer\n",
    "freeze_layers = 12\n",
    "\n",
    "# reload pretrained model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    checkpoint_path,\n",
    "    num_labels=len(target_name_id_dict.keys()),\n",
    "    output_attentions=False,\n",
    "    output_hidden_states=False,\n",
    "    knowledges=knowledges,\n",
    ")\n",
    "\n",
    "if freeze_layers > 0:\n",
    "    modules_to_freeze = model.bert.encoder.layer[:freeze_layers]\n",
    "    for module in modules_to_freeze:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set output dir\n",
    "output_dir='../../down_stream_outputs'\n",
    "# make output directory\n",
    "subprocess.call(f'mkdir {output_dir}', shell=True)\n",
    "\n",
    "# set training arguments\n",
    "training_args = {\n",
    "    # \"run_name\": wandb_name,\n",
    "    \"dataloader_num_workers\": 2,\n",
    "    \"learning_rate\": 5e-5, \n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\", \n",
    "    \"logging_steps\": 10,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": False,\n",
    "    \"lr_scheduler_type\": \"linear\", \n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"num_train_epochs\": 30,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"metric_for_best_model\": \"macro_f1\",\n",
    "    \"greater_is_better\": True,\n",
    "}\n",
    "training_args_init = TrainingArguments(**training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ict01/miniconda3/envs/ict/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    }
   ],
   "source": [
    "# create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args_init,\n",
    "    data_collator=DataCollatorForCellClassification(),\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=test_set,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# train the cell type classifier\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17%|█████████████                                      | 34/200 [00:06<00:34,  4.82it/s]│\n",
      "{'eval_loss': 0.9185497760772705, 'eval_accuracy': 0.8920614998431127, 'eval_precision': 0.7067597099376611, 'eval_recall': 0.702980477684731, 'eval_macro_f1': 0.693570637266355, 'eval_runtime': 41.6667, 'eval_samples_per_second': 76.488, 'eval_steps_per_second': 4.8, 'epoch': 0.0}"
     ]
    }
   ],
   "source": [
    "# test\n",
    "predictions = trainer.predict(test_set)\n",
    "with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(predictions, fp)\n",
    "trainer.save_metrics(\"eval\", predictions.metrics)\n",
    "trainer.save_model(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('geneformer')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fa9502439430aa541f64ed88fbae38bae286936beeca76f67457bee2c04fbf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
